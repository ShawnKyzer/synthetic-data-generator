{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1 - Installing the required dependencies \n",
        "Before we can begin we need to make sure we have all the required dependencies installed in our notebook kernel. You will also want to ensure that you have the configured the correct runtime in the notebook (e.g. GPU or CPU)"
      ],
      "metadata": {
        "id": "Vj5le65hDNRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In order to avoid future dependency issues we have frozen the versions. \n",
        "# This means you may have to alter these as time goes by and new releases\n",
        "# are available. \n",
        "# From https://github.com/gretelai/gretel-synthetics/blob/master/examples/timeseries_dgan.ipynb\n",
        "!pip install gretel-synthetics==0.19.0\n",
        "!pip install gdown==4.6.0\n",
        "!pip install pandas-profiling==3.6.2\n",
        "!pip install matplotlib==3.6.3 \n",
        "\n",
        "# Be sure and restart the kernel after these installs "
      ],
      "metadata": {
        "id": "9ws1jlBY3O6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2 - Persisting models and accessing training data\n",
        "We need a way to persist our models along with an easy way to pull the training set without having to deal with uploading/downloading to a new runtime. This will save a lot of headache and give us the ability to infer the model later. "
      ],
      "metadata": {
        "id": "8eQQie5pEUd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "# Create two new working directories if they do not already exist\n",
        "import os\n",
        "from os import path\n",
        "\n",
        "new_paths = ['/content/drive/MyDrive/synthetic_data_checkpoints/','/content/drive/MyDrive/synthetic_model_training_data/']\n",
        "for p in new_paths:\n",
        "  if path.exists(p) == False:\n",
        "    os.mkdir(p)\n",
        "\n",
        "# IMPORTANT: At this point you will need to upload a text file containing your training data \n",
        "# to the /content/drive/MyDrive/synthetic_model_training_data directory with the name training_set_time_series.csv.\n",
        "# You only have to do this once unless you want to use new training data. "
      ],
      "metadata": {
        "id": "A22oEQDqE1sF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Skip this step if you are using your own data\n",
        "import gdown\n",
        "\n",
        "url = \"https://drive.google.com/file/d/1-6T9a8kCfF0LilygJWUtTvbSBPJRf4aY/view?usp=share_link\"\n",
        "gdown.download(url, new_paths[1]+'training_set_time_series.csv', quiet=False,fuzzy=True)"
      ],
      "metadata": {
        "id": "Dmsjp66KdKfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv(new_paths[1]+'training_set_time_series.csv')\n",
        "\n",
        "# We will need to reformat the date column for analysis\n",
        "train_df['date'] = pd.to_datetime(train_df['date'], format = '%Y-%m-%d')"
      ],
      "metadata": {
        "id": "XR9v-i1CXo-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# where to save the model for later use \n",
        "checkpoint = new_paths[0] + 'synthetic_data_model.bin'\n",
        "\n",
        "# set the numerical only columns you wish to use\n",
        "# if you use the example provided no need to update these \n",
        "column_list = ['mis_and_disinformation', 'mis_and_disinformation_male',  \n",
        "           'mis_and_disinformation_female','myths','myths_female', \n",
        "           'myths_male', 'new_vaccinations_smoothed']\n"
      ],
      "metadata": {
        "id": "qUyb4fqtbJot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3 - The Setup \n",
        "Now that we have a place to put all our data and persist checkpoints lets start by reading in the data and converting our date column in preparation for the training. "
      ],
      "metadata": {
        "id": "Ji82-W_Vj3Zw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "\n",
        "from gretel_synthetics.timeseries_dgan.dgan import DGAN\n",
        "from gretel_synthetics.timeseries_dgan.config import DGANConfig, OutputType\n"
      ],
      "metadata": {
        "id": "RHmT8tZYki4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We don't want all the columns so lets just select a subset we are interested in \n",
        "# lets next create a sensible feature set for training and testing \n",
        "features = train_df[column_list]\n",
        "\n",
        "# remove any NaN\n",
        "features = features.dropna()"
      ],
      "metadata": {
        "id": "mkI5ZKFQZxV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4 - Feature Engineering\n",
        "We now need to prepare our data set by extracting features and reshaping for the dGAN model training. "
      ],
      "metadata": {
        "id": "T2vucAlGmkes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Depending on your use case you may/may not want to include the date column \n",
        "# If you do include this it will also look for seasonal patterns, but remember\n",
        "# to convert the date to a number as dGAN only accepts numerical fields \n",
        "features = features.to_numpy()\n",
        "\n",
        "# Obsevations every 1 day\n",
        "# Observations per day in the set (such as a sensor data set)\n",
        "obs_per_day = 1\n",
        "n = features.shape[0]\n",
        "features = features[:(n*obs_per_day),:].reshape(-1, obs_per_day, features.shape[1])\n",
        "\n",
        "# Shape is now (# examples, # time points, # features)\n",
        "print(features.shape)"
      ],
      "metadata": {
        "id": "DRifhFGKmh6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5 - Training the model\n",
        "We are now ready to configure the model and begin the training using DGAN and batch training of the dataframe. "
      ],
      "metadata": {
        "id": "7w5lofkrnUvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Recommended to train with a GPU\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "T2jXO1m9nTGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train DGAN model \n",
        "# lets talk about each training parameter in more detail. \n",
        "model = DGAN(DGANConfig(\n",
        "    max_sequence_len=features.shape[1],\n",
        "    sample_len=1, # must be multiple of obs_per_day\n",
        "    batch_size=min(1000, features.shape[0]),\n",
        "    apply_feature_scaling=True,\n",
        "    apply_example_scaling=False,\n",
        "    use_attribute_discriminator=False,\n",
        "    generator_learning_rate=1e-4,\n",
        "    discriminator_learning_rate=1e-4,\n",
        "    epochs=100,\n",
        "))\n",
        "\n",
        "# We have only chosen 100 epochs, but it is likely that you will have to experiment\n",
        "# with say 10,000 but that will take some time to train (~2 hours)\n",
        "\n",
        "model.train_numpy(\n",
        "    features,\n",
        "    feature_types=[OutputType.CONTINUOUS] * features.shape[2],\n",
        ")\n",
        "\n",
        "# Generate synthetic data\n",
        "_, synthetic_features = model.generate_numpy(features.shape[0])"
      ],
      "metadata": {
        "id": "kHRcQ39fn2vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6 - Model Evaluation - How did we do?\n",
        "Now that we have both our initial training set and our generated set lets do a side by side comparision with pandas_profiling. "
      ],
      "metadata": {
        "id": "LWHMOWJrn9n3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets move from 3D to 2D with our original columns\n",
        "synthetic_df = pd.DataFrame(synthetic_features.reshape(-1, synthetic_features.shape[2]), columns=column_list)\n"
      ],
      "metadata": {
        "id": "ye4F1Mi4oNXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pandas_profiling import ProfileReport\n",
        "\n",
        "original_features_df = train_df[column_list]\n",
        "\n",
        "# Produce the data profiling report\n",
        "original_report = ProfileReport(original_features_df, title='Train Data')\n",
        "\n",
        "synthetic_report = ProfileReport(synthetic_df, title='Synthetic Data')\n",
        "\n",
        "comparison_report = original_report.compare(synthetic_report)\n"
      ],
      "metadata": {
        "id": "KtFD0I0ZeSKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save your model an a copy of the report so we can take a look at the \n",
        "# comparision\n",
        "\n",
        "comparison_report.to_file(\"original_vs_transformed.html\") \n",
        "model.save(checkpoint)"
      ],
      "metadata": {
        "id": "KFpCPeMx6xVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now lets have a look at the results \n",
        "comparison_report.to_notebook_iframe()"
      ],
      "metadata": {
        "id": "ZeJ5OFNIiWGW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}